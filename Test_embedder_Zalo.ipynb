{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed6be7b",
   "metadata": {},
   "source": [
    "# Import libraries & parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c82b69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.embedder import SpeechEmbedder\n",
    "from datasets.ZaloAIDataset import create_dataset\n",
    "from utils.hparams import HParam\n",
    "from utils.eer import EER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b99fb",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060b484",
   "metadata": {},
   "source": [
    "Get all folder paths (speaker based). Format will be a single list of folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/voicefilter/utils/hparams.py:18: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "hp = HParam(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4534eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def test_cuda_collate_fn(batch):\n",
    "    w1_mel_list = list()\n",
    "    w2_mel_list = list()\n",
    "    label_list = list()\n",
    "    \n",
    "    for _, _, _, _, w1_mel, w2_mel, label, *_ in batch:\n",
    "        w1_mel_list.append(w1_mel)\n",
    "        w2_mel_list.append(w2_mel)\n",
    "        label_list.append(label)\n",
    "    w1_mel_list = pad_sequence(w1_mel_list, batch_first=True)\n",
    "    w2_mel_list = pad_sequence(w2_mel_list, batch_first=True)\n",
    "    label_list = torch.stack(label_list, dim=0)\n",
    "\n",
    "    return w1_mel_list, w2_mel_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(hp, \"test\")\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=test_cuda_collate_fn,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            sampler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b7121",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechEmbedder(\n",
       "  (lstm): LSTM(40, 768, num_layers=3, batch_first=True)\n",
       "  (proj): LinearNorm(\n",
       "    (linear_layer): Linear(in_features=768, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_pt = torch.load('embedder.pt',map_location=\"cpu\")\n",
    "embedder = SpeechEmbedder(hp)\n",
    "embedder.load_state_dict(embedder_pt)\n",
    "embedder = embedder.cuda()\n",
    "embedder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('datasets/ZaloAI2020/private-test/0/0/q0X0yY4gYC6jt0rYOcwY.wav',\n",
       " 'datasets/ZaloAI2020/private-test/0/0/kjeWQBJ70qDLk7R7M7Ih.wav',\n",
       " array([ 3.0029297e-02,  6.2164307e-02,  7.3486328e-02, ...,\n",
       "        -6.1035156e-03, -5.1574707e-03, -3.0517578e-05], dtype=float32),\n",
       " array([ 0.01809692,  0.02774048,  0.02139282, ...,  0.03521729,\n",
       "         0.01431274, -0.00427246], dtype=float32),\n",
       " tensor([[-0.6013, -2.2244, -2.2547,  ..., -2.1950, -2.8161, -2.4075],\n",
       "         [ 0.0330, -0.6561, -1.1946,  ..., -0.9638, -1.1659, -1.5474],\n",
       "         [ 0.4425,  0.3706, -0.3333,  ..., -0.4274, -0.6720, -1.1104],\n",
       "         ...,\n",
       "         [-4.7263, -5.8529, -5.6487,  ..., -5.7564, -5.6639, -5.6716],\n",
       "         [-4.5932, -5.8565, -5.8931,  ..., -5.7741, -5.7262, -5.6187],\n",
       "         [-4.4717, -5.7999, -5.8866,  ..., -5.7750, -5.7579, -5.8008]]),\n",
       " tensor([[-1.6268, -1.7113, -2.4918,  ..., -1.8787, -1.8889, -0.1788],\n",
       "         [-0.8937, -1.8994, -1.8591,  ..., -0.5464, -0.4369,  0.5396],\n",
       "         [-0.2642, -0.1607, -0.0250,  ...,  1.0127,  1.0251,  0.7308],\n",
       "         ...,\n",
       "         [-5.3420, -5.7677, -5.6654,  ..., -5.3752, -5.5456, -5.3621],\n",
       "         [-5.2526, -5.8415, -5.8062,  ..., -5.5914, -5.5708, -4.9554],\n",
       "         [-5.0058, -5.8239, -5.8549,  ..., -5.2840, -5.3470, -5.2627]]),\n",
       " 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_path, s2_path, w1, w2, m1, m2, label = dataset[0]\n",
    "s1_path, s2_path, w1, w2, m1, m2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.31 ms, sys: 6.81 ms, total: 11.1 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    e1 = embedder(m1.cuda(non_blocking=True))\n",
    "    e2 = embedder(m2.cuda(non_blocking=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7805, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "cos(e1, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/venv/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "eer = EER(compute_on_step=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [15:42<00:00, 53.03it/s]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    _, _, _, _, m1, m2, label = dataset[i]\n",
    "    with torch.no_grad():\n",
    "        e1 = embedder(m1.cuda(non_blocking=True))\n",
    "        e2 = embedder(m2.cuda(non_blocking=True))\n",
    "    sim = cos(e1, e2).cpu()\n",
    "    pred.append(sim.item())\n",
    "    eer(sim.reshape((1, 1)), torch.tensor(label).reshape((1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/venv/lib/python3.8/site-packages/scipy/interpolate/interpolate.py:630: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/root/.virtualenvs/venv/lib/python3.8/site-packages/scipy/interpolate/interpolate.py:633: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, array(nan))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(dataset.data[\"label\"], pred, pos_label=1)\n",
    "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "thresh = interp1d(fpr, thresholds)(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16911323547058113, array(0.51920239))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83072"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset.data[\"label\"] == (pred > 0.519)*1)/len(pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ed27c33f293ca8c3697f2b328444ff97f02b44e95d63e9588b837d1a5300422"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
