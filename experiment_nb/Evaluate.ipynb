{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82b69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from utils.hparams import HParam\n",
    "from utils.writer import MyWriter\n",
    "from datasets.dataloader import create_dataloader\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-b', '--base_dir', type=str, default='.',\n",
    "                    help=\"Root directory of run.\")\n",
    "parser.add_argument('-c', '--config', type=str, required=True,\n",
    "                    help=\"yaml file for configuration\")\n",
    "parser.add_argument('-e', '--embedder_path', type=str, required=True,\n",
    "                    help=\"path of embedder model pt file\")\n",
    "parser.add_argument('--checkpoint_path', type=str, default=None,\n",
    "                    help=\"path of checkpoint pt file\")\n",
    "parser.add_argument('-m', '--model', type=str, required=True,\n",
    "                    help=\"Name of the model. Used for both logging and saving checkpoints.\")\n",
    "args = parser.parse_args([\"-c\", \"config.yaml\", \"-e\", \"embedder.pt\", \"-m\", \"eval\"])\n",
    "\n",
    "hp = HParam(args.config)\n",
    "with open(args.config, 'r') as f:\n",
    "    # store hparams as string\n",
    "    hp_str = ''.join(f.readlines())\n",
    "\n",
    "pt_dir = os.path.join(args.base_dir, hp.log.chkpt_dir, args.model)\n",
    "os.makedirs(pt_dir, exist_ok=True)\n",
    "\n",
    "log_dir = os.path.join(args.base_dir, hp.log.log_dir, args.model)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "chkpt_path = args.checkpoint_path if args.checkpoint_path is not None else None\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir,\n",
    "            '%s-%d.log' % (args.model, time.time()))),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "if hp.data.train_dir == '' or hp.data.test_dir == '':\n",
    "    logger.error(\"train_dir, test_dir cannot be empty.\")\n",
    "    raise Exception(\"Please specify directories of data in %s\" % args.config)\n",
    "\n",
    "writer = MyWriter(hp, log_dir)\n",
    "\n",
    "trainloader = create_dataloader(hp, args, train=True)\n",
    "testloader = create_dataloader(hp, args, train=False)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "\n",
    "from utils.adabound import AdaBound\n",
    "from utils.audio import Audio\n",
    "from utils.evaluation import validate\n",
    "from model.model import VoiceFilter\n",
    "from model.embedder import SpeechEmbedder\n",
    "from utils.power_law_loss import PowerLawCompLoss\n",
    "from utils.gdrive import GDrive\n",
    "\n",
    "# load embedder\n",
    "embedder_pt = torch.load(args.embedder_path)\n",
    "embedder = SpeechEmbedder(hp).cuda()\n",
    "embedder.load_state_dict(embedder_pt)\n",
    "embedder.eval()\n",
    "\n",
    "audio = Audio(hp)\n",
    "model = VoiceFilter(hp).cuda()\n",
    "if hp.train.optimizer == 'adabound':\n",
    "    optimizer = AdaBound(model.parameters(),\n",
    "                         lr=hp.train.adabound.initial,\n",
    "                         final_lr=hp.train.adabound.final)\n",
    "elif hp.train.optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=hp.train.adam)\n",
    "else:\n",
    "    raise Exception(\"%s optimizer not supported\" % hp.train.optimizer)\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc6c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_path = \"chkpt/final_try/chkpt_66000.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd867222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 08:31:21,665 - INFO - Resuming from checkpoint: chkpt/final_try/chkpt_66000.pt\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Resuming from checkpoint: %s\" % chkpt_path)\n",
    "checkpoint = torch.load(chkpt_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "step = checkpoint['step']\n",
    "\n",
    "# will use new given hparams.\n",
    "if hp_str != checkpoint['hp_str']:\n",
    "    logger.warning(\"New hparams is different from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12056d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "_criterion = PowerLawCompLoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "accum = 0\n",
    "accum_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90316747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mir_eval.separation import bss_eval_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c67d828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        dvec_mel, target_wav, mixed_wav, target_mag, mixed_mag, mixed_phase = batch[0]\n",
    "\n",
    "        dvec_mel = dvec_mel.cuda()\n",
    "        target_mag = target_mag.unsqueeze(0).cuda()\n",
    "        mixed_mag = mixed_mag.unsqueeze(0).cuda()\n",
    "\n",
    "        dvec = embedder(dvec_mel)\n",
    "        dvec = dvec.unsqueeze(0)\n",
    "        est_mask = model(mixed_mag, dvec)\n",
    "        est_mag = est_mask * mixed_mag\n",
    "        test_loss = criterion(target_mag, est_mag).item()\n",
    "\n",
    "        mixed_mag = mixed_mag[0].cpu().detach().numpy()\n",
    "        target_mag = target_mag[0].cpu().detach().numpy()\n",
    "        est_mag = est_mag[0].cpu().detach().numpy()\n",
    "        est_wav = audio.spec2wav(est_mag, mixed_phase)\n",
    "        est_mask = est_mask[0].cpu().detach().numpy()\n",
    "\n",
    "        sdr = bss_eval_sources(target_wav, est_wav, False)[0][0]\n",
    "        writer.log_evaluation(test_loss, sdr,\n",
    "                              mixed_wav, target_wav, est_wav,\n",
    "                              mixed_mag.T, target_mag.T, est_mag.T, est_mask.T,\n",
    "                              step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dvec_mels, target_mag, mixed_mag in trainloader:\n",
    "    target_mag = target_mag.cuda()\n",
    "    mixed_mag = mixed_mag.cuda()\n",
    "    \n",
    "    dvec_list = list()\n",
    "    for mel in dvec_mels:\n",
    "        mel = mel.cuda()\n",
    "        dvec = embedder(mel)\n",
    "        dvec_list.append(dvec)\n",
    "    dvec = torch.stack(dvec_list, dim=0)\n",
    "    dvec = dvec.detach()\n",
    "    \n",
    "    mask = model(mixed_mag, dvec)\n",
    "    output = mixed_mag * mask\n",
    "\n",
    "    # output = torch.pow(torch.clamp(output, min=0.0), hp.audio.power)\n",
    "    # target_mag = torch.pow(torch.clamp(target_mag, min=0.0), hp.audio.power)\n",
    "    loss = criterion(output, target_mag)\n",
    "\n",
    "    loss.backward()\n",
    "    accum_loss += loss.item()\n",
    "    accum += 1\n",
    "    \n",
    "    if accum % hp[\"train\"][\"grad_accumulate\"] == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        accum = 0\n",
    "        step += 1\n",
    "        accum_loss /= hp[\"train\"][\"grad_accumulate\"]\n",
    "        \n",
    "        if accum_loss > 1e8 or math.isnan(accum_loss):\n",
    "            logger.error(\"Loss exploded to %.02f at step %d!\" % (accum_loss, step))\n",
    "            raise Exception(\"Loss exploded\")\n",
    "\n",
    "        if step == 2100:\n",
    "            break\n",
    "\n",
    "        # write loss to tensorboard\n",
    "        if step % hp.train.summary_interval == 0:\n",
    "            writer.log_training(accum_loss, step)\n",
    "            logger.info(\"Wrote summary at step %d\" % step)\n",
    "\n",
    "        accum_loss = 0\n",
    "            \n",
    "        # 1. save checkpoint file to resume training\n",
    "        # 2. evaluate and save sample to tensorboard\n",
    "        # backup brrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
    "        if step % hp.train.checkpoint_interval == 0:\n",
    "            save_path = os.path.join(pt_dir, 'chkpt_%d.pt' % step)\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'step': step,\n",
    "                'hp_str': hp_str,\n",
    "            }, save_path)\n",
    "            logger.info(\"Saved checkpoint to: %s\" % save_path)\n",
    "            validate(audio, model, embedder, testloader, writer, step)\n",
    "\n",
    "            # drive.Upload(save_path, \"1sWAUt5vfyD97Cq85J8_zuwMeX4tmfEiZ\")\n",
    "            asyncio.run(UploadToDrive(drive, save_path))\n",
    "\n",
    "            # NÃ©n file\n",
    "            os.system(f'zip -j ./tensorboard.zip ./{log_dir}/*')\n",
    "            drive.Upload('tensorboard.zip', \"1sWAUt5vfyD97Cq85J8_zuwMeX4tmfEiZ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
