{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed6be7b",
   "metadata": {},
   "source": [
    "# Import libraries & parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82b69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import soundfile\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from utils.audio import Audio\n",
    "from utils.hparams import HParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a49c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/voicefilter/utils/hparams.py:18: YAMLLoadWarning: calling yaml.load_all() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  for doc in docs:\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-c', '--config', type=str, required=True,\n",
    "                    help=\"yaml file for configuration\")\n",
    "parser.add_argument('-d', '--libri_dir', type=str, default=None,\n",
    "                    help=\"Directory of LibriSpeech dataset, containing folders of train-clean-100, train-clean-360, dev-clean.\")\n",
    "parser.add_argument('-v', '--voxceleb_dir', type=str, default=None,\n",
    "                    help=\"Directory of VoxCeleb2 dataset, ends with 'aac'\")\n",
    "parser.add_argument('-o', '--out_dir', type=str, required=True,\n",
    "                    help=\"Directory of output training triplet\")\n",
    "parser.add_argument('-p', '--process_num', type=int, default=None,\n",
    "                    help='number of processes to run. default: cpu_count')\n",
    "parser.add_argument('--vad', type=int, default=0,\n",
    "                    help='apply vad to wav file. yes(1) or no(0, default)')\n",
    "parser.add_argument('--train_amt', type=int, default=4,\n",
    "                    help='specify the amount of mixed train data (default is 4, equal to 10**4)')\n",
    "args = parser.parse_args([\"-c\", \"config.yaml\", \"-o\", \"tmp_gen\", \"-d\", \"datasets/LibriSpeech\"])\n",
    "hp = HParam(args.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b99fb",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6076c3",
   "metadata": {},
   "source": [
    "Make output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736a25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.out_dir, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(args.out_dir, 'test'), exist_ok=True)\n",
    "\n",
    "cpu_num = cpu_count() if args.process_num is None else args.process_num\n",
    "\n",
    "if args.libri_dir is None and args.voxceleb_dir is None:\n",
    "    raise Exception(\"Please provide directory of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060b484",
   "metadata": {},
   "source": [
    "Get all folder paths (speaker based). Format will be a single list of folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4534eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.libri_dir is not None:\n",
    "    train_folders = [x for x in glob.glob(os.path.join(args.libri_dir, 'train-clean-100', '*'))\n",
    "                        if os.path.isdir(x)] + \\\n",
    "                    [x for x in glob.glob(os.path.join(args.libri_dir, 'train-clean-360', '*'))\n",
    "                        if os.path.isdir(x)]\n",
    "                    # we recommned to exclude train-other-500\n",
    "                    # See https://github.com/mindslab-ai/voicefilter/issues/5#issuecomment-497746793\n",
    "                    # + \\\n",
    "                    #[x for x in glob.glob(os.path.join(args.libri_dir, 'train-other-500', '*'))\n",
    "                    #    if os.path.isdir(x)]\n",
    "    test_folders = [x for x in glob.glob(os.path.join(args.libri_dir, 'dev-clean', '*'))]\n",
    "\n",
    "elif args.voxceleb_dir is not None:\n",
    "    all_folders = [x for x in glob.glob(os.path.join(args.voxceleb_dir, '*'))\n",
    "                        if os.path.isdir(x)]\n",
    "    train_folders = all_folders[:-20]\n",
    "    test_folders = all_folders[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9adc5",
   "metadata": {},
   "source": [
    "Get all audio file for each speaker. Then remove all speakers who have less than 2 audio files. Format will be [speaker0, speaker1,...] where speakerx = [audiopath0, audiopath1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894105f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spk = [glob.glob(os.path.join(spk, '**', hp.form.input), recursive=True)\n",
    "                for spk in train_folders]\n",
    "train_spk = [x for x in train_spk if len(x) >= 2]\n",
    "\n",
    "test_spk = [glob.glob(os.path.join(spk, '**', hp.form.input), recursive=True)\n",
    "                for spk in test_folders]\n",
    "test_spk = [x for x in test_spk if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31581ec",
   "metadata": {},
   "source": [
    "Audio is an abstract class that help simplify many operation on a single audio file like convert to mel, waveform to mel or mel to waveform,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e86698b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b7121",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2596013",
   "metadata": {},
   "source": [
    "Function that generate path of new file with format: `dir_/num-target.wav`\n",
    "<br>(form is specified in yaml as \"*-target.wav\", this function will replace * with num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7612a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatter(dir_, form, num):\n",
    "    return os.path.join(dir_, form.replace('*', '%06d' % num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfac231",
   "metadata": {},
   "source": [
    "This function will cut off all segment that considered as silence (db <= 20) from an audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8528d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad_merge(w):\n",
    "    intervals = librosa.effects.split(w, top_db=20)\n",
    "    temp = list()\n",
    "    for s, e in intervals:\n",
    "        temp.append(w[s:e])\n",
    "    return np.concatenate(temp, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458facaf",
   "metadata": {},
   "source": [
    "This function will perform:\n",
    "- Take 3 mono audio, trim leading and trailing silence\n",
    "- Skip when getting short audio reference for d-vector\n",
    "- Cut-off all silence segment from these audios, then take the first 3 second from these audio -> the repo author said that to get the same sdr, dont do this, I also think of the same thing too. But he also state that librispeech have many silence interval. Hmm...\n",
    "- Normalize them (divide by max(abs)*1.1)\n",
    "- Save normalized audio (target and mixed), then save spectrogram (torch.save for target and mixed), and save d-vector audio path as text file. All these saved file will have the same number on them.\n",
    "\n",
    "Params:\n",
    "- hp, args, audio: config and audio class\n",
    "- num: output file number\n",
    "- s1_dvec: audio for d-vector\n",
    "- s1_target: audio used as target\n",
    "- s2: audio is used to mix with s1_target, result in mixed audio \n",
    "- train: used to specify if output is test or train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e21ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix(hp, args, audio, num, s1_dvec, s1_target, s2, train):\n",
    "    srate = hp.audio.sample_rate\n",
    "    dir_ = os.path.join(args.out_dir, 'train' if train else 'test')\n",
    "\n",
    "    d, _ = librosa.load(s1_dvec, sr=srate)\n",
    "    w1, _ = librosa.load(s1_target, sr=srate)\n",
    "    w2, _ = librosa.load(s2, sr=srate)\n",
    "    assert len(d.shape) == len(w1.shape) == len(w2.shape) == 1, \\\n",
    "        'wav files must be mono, not stereo'\n",
    "\n",
    "    d, _ = librosa.effects.trim(d, top_db=20)\n",
    "    w1, _ = librosa.effects.trim(w1, top_db =20)\n",
    "    w2, _ = librosa.effects.trim(w2, top_db=20)\n",
    "\n",
    "    # if reference for d-vector is too short, discard it\n",
    "    if d.shape[0] < 1.1 * hp.embedder.window * hp.audio.hop_length:\n",
    "        return\n",
    "\n",
    "    # LibriSpeech dataset have many silent interval, so let's vad-merge them\n",
    "    # VoiceFilter paper didn't do that. To test SDR in same way, don't vad-merge.\n",
    "    if args.vad == 1:\n",
    "        w1, w2 = vad_merge(w1), vad_merge(w2)\n",
    "\n",
    "    # I think random segment length will be better, but let's follow the paper first\n",
    "    # fit audio to `hp.data.audio_len` seconds.\n",
    "    # if merged audio is shorter than `L`, discard it\n",
    "    L = int(srate * hp.data.audio_len)\n",
    "    if w1.shape[0] < L or w2.shape[0] < L:\n",
    "        return\n",
    "    w1, w2 = w1[:L], w2[:L]\n",
    "\n",
    "    mixed = w1 + w2\n",
    "\n",
    "    norm = np.max(np.abs(mixed)) * 1.1\n",
    "    w1, w2, mixed = w1/norm, w2/norm, mixed/norm\n",
    "\n",
    "    # save vad & normalized wav files\n",
    "    target_wav_path = formatter(dir_, hp.form.target.wav, num)\n",
    "    mixed_wav_path = formatter(dir_, hp.form.mixed.wav, num)\n",
    "    soundfile.write(target_wav_path, w1, srate)\n",
    "    soundfile.write(mixed_wav_path, mixed, srate)\n",
    "\n",
    "    # save magnitude spectrograms\n",
    "    target_mag, _ = audio.wav2spec(w1)\n",
    "    mixed_mag, _ = audio.wav2spec(mixed)\n",
    "    target_mag_path = formatter(dir_, hp.form.target.mag, num)\n",
    "    mixed_mag_path = formatter(dir_, hp.form.mixed.mag, num)\n",
    "    torch.save(torch.from_numpy(target_mag), target_mag_path)\n",
    "    torch.save(torch.from_numpy(mixed_mag), mixed_mag_path)\n",
    "\n",
    "    # save selected sample as text file. d-vec will be calculated soon\n",
    "    dvec_text_path = formatter(dir_, hp.form.dvec, num)\n",
    "    with open(dvec_text_path, 'w') as f:\n",
    "        f.write(s1_dvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a9b30",
   "metadata": {},
   "source": [
    "This is wrapper function before perform multi-processing, get 2 random speaker. For speaker 1, get 2 random audio, 1 for d-vector, and 1 for target. For speaker 2 get a random audio to mix with target audio. Then perform mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce44d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(num):\n",
    "    spk1, spk2 = random.sample(train_spk, 2)\n",
    "    s1_dvec, s1_target = random.sample(spk1, 2)\n",
    "    s2 = random.choice(spk2)\n",
    "    mix(hp, args, audio, num, s1_dvec, s1_target, s2, train=True)\n",
    "\n",
    "def test_wrapper(num):\n",
    "    spk1, spk2 = random.sample(test_spk, 2)\n",
    "    s1_dvec, s1_target = random.sample(spk1, 2)\n",
    "    s2 = random.choice(spk2)\n",
    "    mix(hp, args, audio, num, s1_dvec, s1_target, s2, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37158e17",
   "metadata": {},
   "source": [
    "Generate 10k train data, and 100 test data. In the original repo, author generate 100k train data instead of 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f1b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 211.62it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 369.45it/s]\n"
     ]
    }
   ],
   "source": [
    "arr = list(range(10**2))\n",
    "with Pool(cpu_num) as p:\n",
    "   r = list(tqdm.tqdm(p.imap(train_wrapper, arr), total=len(arr)))\n",
    "\n",
    "arr = list(range(10**2))\n",
    "with Pool(cpu_num) as p:\n",
    "    r = list(tqdm.tqdm(p.imap(test_wrapper, arr), total=len(arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00358f6a",
   "metadata": {},
   "source": [
    "Profiling time for getting each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c0a87cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5 ms ± 1.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_wrapper(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4769f32",
   "metadata": {},
   "source": [
    "Without saving (true time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34315d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix(hp, args, audio, num, s1_dvec, s1_target, s2, train):\n",
    "    srate = hp.audio.sample_rate\n",
    "    dir_ = os.path.join(args.out_dir, 'train' if train else 'test')\n",
    "\n",
    "    d, _ = librosa.load(s1_dvec, sr=srate)\n",
    "    w1, _ = librosa.load(s1_target, sr=srate)\n",
    "    w2, _ = librosa.load(s2, sr=srate)\n",
    "    assert len(d.shape) == len(w1.shape) == len(w2.shape) == 1, \\\n",
    "        'wav files must be mono, not stereo'\n",
    "\n",
    "    d, _ = librosa.effects.trim(d, top_db=20)\n",
    "    w1, _ = librosa.effects.trim(w1, top_db =20)\n",
    "    w2, _ = librosa.effects.trim(w2, top_db=20)\n",
    "\n",
    "    # if reference for d-vector is too short, discard it\n",
    "    if d.shape[0] < 1.1 * hp.embedder.window * hp.audio.hop_length:\n",
    "        return\n",
    "\n",
    "    # LibriSpeech dataset have many silent interval, so let's vad-merge them\n",
    "    # VoiceFilter paper didn't do that. To test SDR in same way, don't vad-merge.\n",
    "    if args.vad == 1:\n",
    "        w1, w2 = vad_merge(w1), vad_merge(w2)\n",
    "\n",
    "    # I think random segment length will be better, but let's follow the paper first\n",
    "    # fit audio to `hp.data.audio_len` seconds.\n",
    "    # if merged audio is shorter than `L`, discard it\n",
    "    L = int(srate * hp.data.audio_len)\n",
    "    if w1.shape[0] < L or w2.shape[0] < L:\n",
    "        return\n",
    "    w1, w2 = w1[:L], w2[:L]\n",
    "\n",
    "    mixed = w1 + w2\n",
    "\n",
    "    norm = np.max(np.abs(mixed)) * 1.1\n",
    "    w1, w2, mixed = w1/norm, w2/norm, mixed/norm\n",
    "\n",
    "    # save vad & normalized wav files\n",
    "    target_wav_path = formatter(dir_, hp.form.target.wav, num)\n",
    "    mixed_wav_path = formatter(dir_, hp.form.mixed.wav, num)\n",
    "    soundfile.write(target_wav_path, w1, srate)\n",
    "    soundfile.write(mixed_wav_path, mixed, srate)\n",
    "\n",
    "    # save magnitude spectrograms\n",
    "    target_mag, _ = audio.wav2spec(w1)\n",
    "    mixed_mag, _ = audio.wav2spec(mixed)\n",
    "\n",
    "    dvec_mel = audio.get_mel(d)\n",
    "    dvec_mel = torch.from_numpy(dvec_mel).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4db67c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.2 ms ± 4.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_wrapper(int(time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
