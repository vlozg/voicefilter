experiment:
  name: ""
  clean_rerun: True # Delete old logs and chkpt if true
  use_cuda: True
  
  data:
    train_set: ["librispeech-train"]
    eval_set: ["librispeech-test"]
    test_set: ["librispeech-test"]
    audio_len: 3.0
  
  model:
    name: "voicefilter"
    pretrained_chkpt:
    resume_from_chkpt: False
    input_dim:  # 8*audio.num_freq + embedder.emb_dim,  will get computed in get_model.py
    lstm_dim: 400
    fc1_dim: 600
    fc2_dim: 601 # num_freq
    bidirection: True

  loss_function: "power_law_compressed"

  train:
    use_cuda:
    batch_size: 3
    num_workers: 3
    grad_accumulate: 20
    optimizer: "adam"
    optimizer_param: 
      lr: 0.001
    max_step: 500000
    summary_interval: 1
    checkpoint_interval: 3000
  
  embedder: # d-vector embedder. don"t fix it!
    name: "ZaloTop1" # GE2E or ZaloTop1
    use_cuda: False
    emb_dim: 512
    num_mels: 40
    n_fft: 512
  
  audio:
    n_fft: 1200
    num_freq: 601 # n_fft//2 + 1
    sample_rate: 16000
    hop_length: 160
    win_length: 400
    min_level_db: -100.0
    ref_level_db: 20.0
    preemphasis: 0.97
    power: 0.30

---
env:
  base_dir: "."
  data:
    libri_dir: "datasets/LibriSpeech"
    vivos_dir: "datasets/VIVOS/vivos"
    vctk_dir: "datasets/VCTK-Corpus"
    voxceleb1_dir: "datasets/VoxCeleb1"
    voxceleb2_dir: "datasets/VoxCeleb2"
    vin_dir: "datasets/VinBigdata"
    zalo_dir: "datasets/ZaloAI2020"

  log:
    chkpt_dir: "chkpt"
    log_dir: "logs"