{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed6be7b",
   "metadata": {},
   "source": [
    "# Import libraries & parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c82b69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import librosa\n",
    "import argparse\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.audio import Audio\n",
    "from utils.hparams import HParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a49c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-c', '--config', type=str, required=True,\n",
    "                    help=\"yaml file for configuration\")\n",
    "parser.add_argument('-d', '--libri_dir', type=str, default=None,\n",
    "                    help=\"Directory of LibriSpeech dataset, containing folders of train-clean-100, train-clean-360, dev-clean.\")\n",
    "parser.add_argument('-v', '--voxceleb_dir', type=str, default=None,\n",
    "                    help=\"Directory of VoxCeleb2 dataset, ends with 'aac'\")\n",
    "parser.add_argument('-o', '--out_dir', type=str, required=True,\n",
    "                    help=\"Directory of output training triplet\")\n",
    "parser.add_argument('-p', '--process_num', type=int, default=None,\n",
    "                    help='number of processes to run. default: cpu_count')\n",
    "parser.add_argument('--vad', type=int, default=0,\n",
    "                    help='apply vad to wav file. yes(1) or no(0, default)')\n",
    "parser.add_argument('--train_amt', type=int, default=4,\n",
    "                    help='specify the amount of mixed train data (default is 4, equal to 10**4)')\n",
    "args = parser.parse_args([\"-c\", \"config.yaml\", \"-o\", \"tmp_gen\", \"-d\", \"datasets/LibriSpeech\"])\n",
    "hp = HParam(args.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b99fb",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060b484",
   "metadata": {},
   "source": [
    "Get all folder paths (speaker based). Format will be a single list of folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4534eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.libri_dir is not None:\n",
    "    train_folders = [x for x in glob.glob(os.path.join(args.libri_dir, 'train-clean-100', '*'))\n",
    "                        if os.path.isdir(x)] + \\\n",
    "                    [x for x in glob.glob(os.path.join(args.libri_dir, 'train-clean-360', '*'))\n",
    "                        if os.path.isdir(x)]\n",
    "                    # we recommned to exclude train-other-500\n",
    "                    # See https://github.com/mindslab-ai/voicefilter/issues/5#issuecomment-497746793\n",
    "                    # + \\\n",
    "                    #[x for x in glob.glob(os.path.join(args.libri_dir, 'train-other-500', '*'))\n",
    "                    #    if os.path.isdir(x)]\n",
    "    test_folders = [x for x in glob.glob(os.path.join(args.libri_dir, 'dev-clean', '*'))]\n",
    "\n",
    "elif args.voxceleb_dir is not None:\n",
    "    all_folders = [x for x in glob.glob(os.path.join(args.voxceleb_dir, '*'))\n",
    "                        if os.path.isdir(x)]\n",
    "    train_folders = all_folders[:-20]\n",
    "    test_folders = all_folders[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9adc5",
   "metadata": {},
   "source": [
    "Get all audio file for each speaker. Then remove all speakers who have less than 2 audio files. Format will be [speaker0, speaker1,...] where speakerx = [audiopath0, audiopath1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894105f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spk = [glob.glob(os.path.join(spk, '**', hp.form.input), recursive=True)\n",
    "                for spk in train_folders]\n",
    "train_spk = [x for x in train_spk if len(x) >= 2]\n",
    "\n",
    "test_spk = [glob.glob(os.path.join(spk, '**', hp.form.input), recursive=True)\n",
    "                for spk in test_folders]\n",
    "test_spk = [x for x in test_spk if len(x) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31581ec",
   "metadata": {},
   "source": [
    "Audio is an abstract class that help simplify many operation on a single audio file like convert to mel, waveform to mel or mel to waveform,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e86698b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b7121",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfac231",
   "metadata": {},
   "source": [
    "This function will cut off all segment that considered as silence (db <= 20) from an audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8528d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad_merge(w):\n",
    "    intervals = librosa.effects.split(w, top_db=20)\n",
    "    temp = list()\n",
    "    for s, e in intervals:\n",
    "        temp.append(w[s:e])\n",
    "    return np.concatenate(temp, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6873c5",
   "metadata": {},
   "source": [
    "Sample 3 audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce44d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk1, spk2 = random.sample(train_spk, 2)\n",
    "s1_dvec, s1_target = random.sample(spk1, 2)\n",
    "s2 = random.choice(spk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48e21ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "srate = hp.audio.sample_rate\n",
    "\n",
    "d, _ = librosa.load(s1_dvec, sr=srate)\n",
    "w1, _ = librosa.load(s1_target, sr=srate)\n",
    "w2, _ = librosa.load(s2, sr=srate)\n",
    "assert len(d.shape) == len(w1.shape) == len(w2.shape) == 1, \\\n",
    "    'wav files must be mono, not stereo'\n",
    "\n",
    "d, _ = librosa.effects.trim(d, top_db=20)\n",
    "w1, _ = librosa.effects.trim(w1, top_db =20)\n",
    "w2, _ = librosa.effects.trim(w2, top_db=20)\n",
    "\n",
    "# if reference for d-vector is too short, discard it\n",
    "if d.shape[0] < 1.1 * hp.embedder.window * hp.audio.hop_length:\n",
    "    raise\n",
    "\n",
    "# LibriSpeech dataset have many silent interval, so let's vad-merge them\n",
    "# VoiceFilter paper didn't do that. To test SDR in same way, don't vad-merge.\n",
    "if args.vad == 1:\n",
    "    w1, w2 = vad_merge(w1), vad_merge(w2)\n",
    "\n",
    "# I think random segment length will be better, but let's follow the paper first\n",
    "# fit audio to `hp.data.audio_len` seconds.\n",
    "# if merged audio is shorter than `L`, discard it\n",
    "L = int(srate * hp.data.audio_len)\n",
    "if w1.shape[0] < L or w2.shape[0] < L:\n",
    "    raise\n",
    "w1, w2 = w1[:L], w2[:L]\n",
    "\n",
    "mixed = w1 + w2\n",
    "\n",
    "norm = np.max(np.abs(mixed)) * 1.1\n",
    "w1, w2, mixed = w1/norm, w2/norm, mixed/norm\n",
    "\n",
    "target_mag, target_phase = audio.wav2spec(w1)\n",
    "mixed_mag, mixed_phase = audio.wav2spec(mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechEmbedder(\n",
       "  (lstm): LSTM(40, 768, num_layers=3, batch_first=True)\n",
       "  (proj): LinearNorm(\n",
       "    (linear_layer): Linear(in_features=768, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder_pt = torch.load('embedder.pt',map_location=\"cpu\")\n",
    "embedder = SpeechEmbedder(hp)\n",
    "embedder.load_state_dict(embedder_pt)\n",
    "embedder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvec_mel = audio.get_mel(d)\n",
    "dvec_mel = torch.from_numpy(dvec_mel).float()\n",
    "a1=embedder(dvec_mel)\n",
    "\n",
    "w1_mel=audio.get_mel(w1)\n",
    "w1_mel= torch.from_numpy(w1_mel).float()\n",
    "a2=embedder(w1_mel)\n",
    "\n",
    "w2_mel=audio.get_mel(w2)\n",
    "w2_mel= torch.from_numpy(w2_mel).float()\n",
    "b1=embedder(w2_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvec_mel = audio.get_mel(d)\n",
    "dvec_mel = torch.from_numpy(dvec_mel).float()\n",
    "a11=embedder(dvec_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8309, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "cos(a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210432,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ed27c33f293ca8c3697f2b328444ff97f02b44e95d63e9588b837d1a5300422"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
